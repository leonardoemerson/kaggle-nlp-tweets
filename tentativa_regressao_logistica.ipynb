{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solução - Natural Language Processing with Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tratamento inicial dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando bibliotecas para resolução do problema\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lendo dataset de treino\n",
    "train = pd.read_csv('nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observando as primeiras amostras do dataset\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sabemos que a coluna target faz uma classificação binária se o tweet é ou não um desastre, isto é: se target = 1, então esta amostra é um desastre; se target = 0, então esta amostra não é um desastre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0    4342\n",
       "1    3271"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Contando quantos casos são e quantos não são desastres\n",
    "VCtrain=train['target'].value_counts().to_frame()\n",
    "\n",
    "# Fazendo o plot\n",
    "sns.barplot(data=VCtrain,x=VCtrain.index,y=\"target\",palette=\"mako\")\n",
    "VCtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podemos perceber que as quantidades para cada caso não são tão discrepantes, isto é, as quantidades de desastres e não desastres estão balanceadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tamanho do dataset\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Soma de valores faltantes para cada coluna\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como os valores faltantes se concentram em 'location' e 'keyword', que são campos onde se torna difícil estipular valores, tendo em vista que se trata da localização e da palavra central do tweet, optou-se por colocar espaço vazio nestes campos preservando os textos destas amostras que também são importantes para o dataset de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colocando espaço vazio nos valores faltantes\n",
    "train.fillna('', inplace = True)\n",
    "test.fillna('',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como a coluna id não traz nenhuma informação relevante para a resolução do problema, optou-se por realizar a eliminação desta coluna, assim como as colunas id, keyword e location em test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removendo coluna id de train\n",
    "train.drop(columns=['id'],inplace=True)\n",
    "#Removendo colunas id keyword e location de test\n",
    "test.drop(columns=['id','keyword','location'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removendo pontuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo função para remover pontuação dos textos\n",
    "def remove_pontuacao(text):\n",
    "    livre_de_pontuacao=\"\".join([i for i in text if i not in string.punctuation])\n",
    "    return livre_de_pontuacao\n",
    "#Aplicando a função remove_pontuacao na coluna 'text' do dataset com auxílio da expressão lambda\n",
    "train['text']= train['text'].apply(lambda x:remove_pontuacao(x))\n",
    "test['text']= test['text'].apply(lambda x:remove_pontuacao(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformando o texto em caixa baixa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colocando as amostras de texto da coluna 'text' em caixa baixa com auxílio da expressão lambda\n",
    "train['text']= train['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo função de tokenização\n",
    "def tokeniza(string):\n",
    "    tokens = string.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando a função de tokenização na coluna 'text' e na coluna 'keyword'\n",
    "train['text']= train['text'].apply(lambda x: tokeniza(x))\n",
    "train['keyword']= train['keyword'].apply(lambda x: tokeniza(x))\n",
    "test['text']= test['text'].apply(lambda x: tokeniza(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  keyword location                                               text  target\n",
       "0      []           [our, deeds, are, the, reason, of, this, earth...       1\n",
       "1      []               [forest, fire, near, la, ronge, sask, canada]       1\n",
       "2      []           [all, residents, asked, to, shelter, in, place...       1\n",
       "3      []           [13000, people, receive, wildfires, evacuation...       1\n",
       "4      []           [just, got, sent, this, photo, from, ruby, ala...       1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando alteração realizada pela função\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "def stemming(text):\n",
    "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
    "    return stem_text\n",
    "train['text']=train['text'].apply(lambda x: stemming(x))\n",
    "test['text']= test['text'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vetorização de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo os tokens para string para colocar no Count Vectorizer\n",
    "train['text_strings'] = train['text'].apply(lambda x: ' '.join([str(elem) for elem in x]))\n",
    "test['text_strings'] = test['text'].apply(lambda x: ' '.join([str(elem) for elem in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(train['text_strings'])\n",
    "x_test = vectorizer.transform(test['text_strings'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparando os conjuntos e treinando com o método"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definindo os valores de treinamento e validação para o método"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com isto transformamos o texto em vetor\n",
    "x_train = X.toarray()\n",
    "x_test = x_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(x_train, train['target'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este conjunto pode ser usado direto no método de treino\n",
    "#x_train = np.array(x_train)\n",
    "#x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conferindo dimensão dos conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5329, 19388)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5329,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ambos com mesma quantidade de observações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinando e testando o método"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(Train_X,Train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score ->  80.29772329246934\n",
      "F1 Score       ->  75.91006423982869\n"
     ]
    }
   ],
   "source": [
    "# Medindo a acurácia e f1\n",
    "print(\"Accuracy Score -> \", accuracy_score(Test_Y, pred)*100)\n",
    "print(\"F1 Score       -> \", f1_score(Test_Y, pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fazendo as predições para a resposta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Colocando as predições no arquivo .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('nlp-getting-started/test.csv')\n",
    "\n",
    "resp = pd.DataFrame({'id': np.array(submission.id), 'target': y_test_pred})\n",
    "\n",
    "from numpy import savetxt\n",
    "resp.to_csv('resposta - regressao logistica.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.79007"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
